{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4626b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d879e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2260699, 151)\n",
      "\n",
      "Sample of data:\n"
     ]
    }
   ],
   "source": [
    "accepted_loan = pd.read_csv('C:/Users/ayan.pathak\\Desktop/lending club loan data project/Data/accepted_2007_to_2018Q4.csv/accepted_2007_to_2018Q4.csv',low_memory=False)\n",
    "\n",
    "print(f\"Shape: {accepted_loan.shape}\")\n",
    "print(f\"\\nSample of data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "288c3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loan_amnt  funded_amnt        term   int_rate  installment grade sub_grade  \\\n",
      "0     3600.0       3600.0   36 months  13.990000   123.029999     C        C4   \n",
      "1    24700.0      24700.0   36 months  11.990000   820.280029     C        C1   \n",
      "2    20000.0      20000.0   60 months  10.780000   432.660004     B        B4   \n",
      "3    35000.0      35000.0   60 months  14.850000   829.900024     C        C5   \n",
      "4    10400.0      10400.0   60 months  22.450001   289.910004     F        F1   \n",
      "\n",
      "  emp_length home_ownership  annual_inc  ... pub_rec_bankruptcies tax_liens  \\\n",
      "0  10+ years       MORTGAGE     55000.0  ...                  0.0       0.0   \n",
      "1  10+ years       MORTGAGE     65000.0  ...                  0.0       0.0   \n",
      "2  10+ years       MORTGAGE     63000.0  ...                  0.0       0.0   \n",
      "3  10+ years       MORTGAGE    110000.0  ...                  0.0       0.0   \n",
      "4    3 years       MORTGAGE    104433.0  ...                  0.0       0.0   \n",
      "\n",
      "  tot_hi_cred_lim total_bal_ex_mort  total_bc_limit  \\\n",
      "0        178050.0            7746.0          2400.0   \n",
      "1        314017.0           39475.0         79300.0   \n",
      "2        218418.0           18696.0          6200.0   \n",
      "3        381215.0           52226.0         62500.0   \n",
      "4        439570.0           95768.0         20300.0   \n",
      "\n",
      "   total_il_high_credit_limit  disbursement_method  issue_d_dt  \\\n",
      "0                     13734.0                 Cash  2015-12-01   \n",
      "1                     24667.0                 Cash  2015-12-01   \n",
      "2                     14877.0                 Cash  2015-12-01   \n",
      "3                     18000.0                 Cash  2015-12-01   \n",
      "4                     88097.0                 Cash  2015-12-01   \n",
      "\n",
      "   earliest_cr_line_dt  credit_history_months  \n",
      "0           2003-08-01                  148.0  \n",
      "1           1999-12-01                  192.0  \n",
      "2           2000-08-01                  184.0  \n",
      "3           2008-09-01                   87.0  \n",
      "4           1998-06-01                  210.0  \n",
      "\n",
      "[5 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "print(accepted_loan.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93e3e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 733.02 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage: {accepted_loan.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c3a5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float64 columns: 2\n",
      "Object columns: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify column types\n",
    "float_cols = accepted_loan.select_dtypes(include=['float64']).columns.tolist()\n",
    "object_cols = accepted_loan.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Float64 columns: {len(float_cols)}\")\n",
    "print(f\"Object columns: {len(object_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3b8b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with >50% missing: 0\n",
      "\n",
      "Top 10 columns with highest missing %:\n"
     ]
    }
   ],
   "source": [
    "missing_summary = accepted_loan.isnull().sum()\n",
    "high_missing = missing_summary[missing_summary > 0.5 * len(accepted_loan)]\n",
    "print(f\"Columns with >50% missing: {len(high_missing)}\")\n",
    "print(\"\\nTop 10 columns with highest missing %:\")\n",
    "for col in high_missing.index[:10]:\n",
    "    missing_pct = (accepted_loan[col].isnull().sum() / len(accepted_loan)) * 100\n",
    "    print(f\"{col}: {missing_pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7163bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory reduced: 6414.66MB → 1788.89MB (72.1%)\n"
     ]
    }
   ],
   "source": [
    "def optimize_memory(df):\n",
    "    \"\"\"Aggressive memory optimization for large datasets\"\"\"\n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # Downcast float64 to float32 (halves memory)\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # Downcast objects to categories where appropriate\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique = df[col].nunique()\n",
    "        if num_unique < 0.1 * len(df):  # Low cardinality\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory reduced: {initial_memory:.2f}MB → {final_memory:.2f}MB ({((initial_memory-final_memory)/initial_memory)*100:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "accepted_loan = optimize_memory(accepted_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e43447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preserving 'id' column for potential internal tracking\n",
      "Dropping 3 columns: ['member_id', 'desc', 'url']\n",
      "New shape: (2260699, 148)\n"
     ]
    }
   ],
   "source": [
    "# Columns to drop immediately\n",
    "immediate_drops = [\n",
    "    'member_id',  # 100% missing\n",
    "    'desc',       # 94.4% missing, text field\n",
    "    'url',        # Unique URLs, not predictive\n",
    "]\n",
    "print(\"Preserving 'id' column for potential internal tracking\")\n",
    "\n",
    "# Check which exist and drop\n",
    "existing_drops = [col for col in immediate_drops if col in accepted_loan.columns]\n",
    "print(f\"Dropping {len(existing_drops)} columns: {existing_drops}\")\n",
    "accepted_loan.drop(columns=existing_drops, inplace=True, errors='ignore')\n",
    "print(f\"New shape: {accepted_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179e5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Potential columns overlapping with rejected dataset: 11\n",
      "Sample overlapping columns: ['loan_amnt', 'emp_title', 'emp_length', 'annual_inc', 'purpose', 'title', 'zip_code', 'addr_state', 'dti', 'annual_inc_joint']\n"
     ]
    }
   ],
   "source": [
    "# Identify columns that have direct counterparts in rejected dataset\n",
    "potential_overlap = []\n",
    "for col in accepted_loan.columns:\n",
    "    col_lower = col.lower()\n",
    "    # Look for feature name patterns that might match rejected loans\n",
    "    if any(term in col_lower for term in ['loan_amnt', 'annual_inc', 'dti', 'zip', 'addr', 'emp', 'title', 'purpose']):\n",
    "        potential_overlap.append(col)\n",
    "\n",
    "print(f\"\\nPotential columns overlapping with rejected dataset: {len(potential_overlap)}\")\n",
    "print(\"Sample overlapping columns:\", potential_overlap[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c5fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 11 potential date columns:\n",
      "1. issue_d: Sample = 'Dec-2015'\n",
      "2. earliest_cr_line: Sample = 'Aug-2003'\n",
      "3. last_pymnt_d: Sample = 'Jan-2019'\n",
      "4. next_pymnt_d: Sample = 'Apr-2019'\n",
      "5. last_credit_pull_d: Sample = 'Mar-2019'\n",
      "6. sec_app_earliest_cr_line: Sample = 'Feb-2005'\n",
      "7. hardship_start_date: Sample = 'Sep-2017'\n",
      "8. hardship_end_date: Sample = 'Dec-2017'\n",
      "9. payment_plan_start_date: Sample = 'Oct-2017'\n",
      "10. debt_settlement_flag_date: Sample = 'Nov-2017'\n",
      "11. settlement_date: Sample = 'Sep-2017'\n"
     ]
    }
   ],
   "source": [
    "date_columns = []\n",
    "date_samples = {}\n",
    "\n",
    "for col in accepted_loan.select_dtypes(include=['object', 'category']).columns:\n",
    "    col_lower = col.lower()\n",
    "    # Broader pattern matching for dates\n",
    "    if any(pattern in col_lower for pattern in ['_d', 'date', 'issue', 'earliest', 'last', 'next', 'start', 'end']):\n",
    "        non_null = accepted_loan[col].dropna()\n",
    "        if not non_null.empty:\n",
    "            sample = non_null.iloc[0]\n",
    "            date_columns.append(col)\n",
    "            date_samples[col] = sample\n",
    "\n",
    "print(f\"\\nFound {len(date_columns)} potential date columns:\")\n",
    "for i, col in enumerate(date_columns[:15]):  # Show first 15\n",
    "    print(f\"{i+1}. {col}: Sample = '{date_samples[col]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f98945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 35 leakage columns\n",
      "\n",
      "Dropping 16 columns with >80% missing\n",
      "First 10 high-missing columns to drop: ['annual_inc_joint', 'dti_joint', 'verification_status_joint', 'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc', 'sec_app_open_acc']\n",
      "\n",
      "Dropping 5 redundant columns: ['funded_amnt_inv', 'out_prncp', 'out_prncp_inv', 'pymnt_plan', 'collections_12_mths_ex_med']\n",
      "\n",
      "Shape after pruning: (2260699, 92)\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Drop leakage columns\n",
    "leakage_cols = [\n",
    "    # Payment & performance\n",
    "    'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
    "    'last_fico_range_high', 'last_fico_range_low',\n",
    "    \n",
    "    # Hardship\n",
    "    'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status',\n",
    "    'deferral_term', 'hardship_amount', 'hardship_start_date',\n",
    "    'hardship_end_date', 'payment_plan_start_date', 'hardship_length',\n",
    "    'hardship_dpd', 'hardship_loan_status',\n",
    "    'orig_projected_additional_accrued_interest',\n",
    "    'hardship_payoff_balance_amount', 'hardship_last_payment_amount',\n",
    "    \n",
    "    # Settlement\n",
    "    'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status',\n",
    "    'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term',\n",
    "    \n",
    "    # Recovery\n",
    "    'recoveries', 'collection_recovery_fee',\n",
    "    \n",
    "    # Total payments (reveal outcomes)\n",
    "    'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee'\n",
    "]\n",
    "\n",
    "# Remove only those that exist\n",
    "existing_leakage = [col for col in leakage_cols if col in accepted_loan.columns]\n",
    "print(f\"Dropping {len(existing_leakage)} leakage columns\")\n",
    "accepted_loan.drop(columns=existing_leakage, inplace=True, errors='ignore')\n",
    "\n",
    "# Phase 2: Drop very high missing columns (>80% threshold)\n",
    "missing_pct = accepted_loan.isnull().sum() / len(accepted_loan)\n",
    "high_missing_cols = missing_pct[missing_pct > 0.80].index.tolist()\n",
    "\n",
    "# But be CAREFUL - keep some important even if missing\n",
    "# Remove from high_missing those we want to keep\n",
    "keep_important = ['mths_since_last_delinq', 'mths_since_last_record']  # Important for credit history\n",
    "high_missing_cols = [col for col in high_missing_cols if col not in keep_important]\n",
    "\n",
    "print(f\"\\nDropping {len(high_missing_cols)} columns with >80% missing\")\n",
    "print(\"First 10 high-missing columns to drop:\", high_missing_cols[:10])\n",
    "accepted_loan.drop(columns=high_missing_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# Phase 3: Drop redundant/calculated columns\n",
    "redundant = [\n",
    "    'funded_amnt_inv', 'out_prncp', 'out_prncp_inv',\n",
    "    'pymnt_plan',  # Likely constant\n",
    "    'collections_12_mths_ex_med'\n",
    "]\n",
    "existing_redundant = [col for col in redundant if col in accepted_loan.columns]\n",
    "print(f\"\\nDropping {len(existing_redundant)} redundant columns: {existing_redundant}\")\n",
    "accepted_loan.drop(columns=existing_redundant, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"\\nShape after pruning: {accepted_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09d8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No joint application columns found, setting all to individual\n",
      "mths_since_last_delinq: 51.2% missing - imputing with large value\n",
      "mths_since_last_major_derog: 74.3% missing - imputing with large value\n",
      "mths_since_last_record: 84.1% missing - imputing with large value\n",
      "mths_since_recent_bc_dlq: 77.0% missing - imputing with large value\n",
      "mths_since_recent_inq: 13.1% missing - imputing with large value\n",
      "mths_since_recent_revol_delinq: 67.3% missing - imputing with large value\n",
      "\n",
      "Shape after informative missing handling: (2260699, 93)\n"
     ]
    }
   ],
   "source": [
    "# Create is_joint_app flag BEFORE dropping joint columns\n",
    "# Check if any joint application field has data\n",
    "joint_indicator_cols = ['annual_inc_joint', 'dti_joint', 'verification_status_joint']\n",
    "existing_joint_cols = [col for col in joint_indicator_cols if col in accepted_loan.columns]\n",
    "\n",
    "if existing_joint_cols:\n",
    "    # Create flag: 1 if ANY joint field is not null, else 0\n",
    "    accepted_loan['is_joint_app'] = accepted_loan[existing_joint_cols[0]].notnull().astype(int)\n",
    "    print(f\"Joint applications: {accepted_loan['is_joint_app'].sum():,} ({accepted_loan['is_joint_app'].mean()*100:.1f}%)\")\n",
    "else:\n",
    "    accepted_loan['is_joint_app'] = 0\n",
    "    print(\"No joint application columns found, setting all to individual\")\n",
    "\n",
    "# Handle informative missingness for credit history columns\n",
    "informative_missing_cols = [\n",
    "    'mths_since_last_delinq',        # 51.2% missing = no recent delinquency\n",
    "    'mths_since_last_major_derog',   # 74.3% missing = no major derogatory\n",
    "    'mths_since_last_record',        # 84.1% missing = no public records\n",
    "    'mths_since_recent_bc_dlq',      # Likely similar pattern\n",
    "    'mths_since_recent_inq',         # Missing = no recent inquiries\n",
    "    'mths_since_recent_revol_delinq' # Missing = no revolving delinquency\n",
    "]\n",
    "\n",
    "for col in informative_missing_cols:\n",
    "    if col in accepted_loan.columns:\n",
    "        missing_pct = accepted_loan[col].isnull().mean() * 100\n",
    "        print(f\"{col}: {missing_pct:.1f}% missing - imputing with large value\")\n",
    "        # Impute with 999 (or max+1) to indicate \"never/very long ago\"\n",
    "        accepted_loan[col] = accepted_loan[col].fillna(999)\n",
    "\n",
    "print(f\"\\nShape after informative missing handling: {accepted_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2072ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing issue_d:\n",
      "  Original format sample: 'Dec-2015'\n",
      "  Converted, 0.0% null after conversion\n",
      "\n",
      "Processing earliest_cr_line:\n",
      "  Original format sample: 'Aug-2003'\n",
      "  Created 'credit_history_months' feature\n",
      "  Converted, 0.0% null after conversion\n",
      "\n",
      "Application Date Range:\n",
      "  Start: 2007-06-01 00:00:00\n",
      "  End: 2018-12-01 00:00:00\n",
      "  Time span: 11.5 years\n",
      "\n",
      "Shape after date processing: (2260699, 94)\n"
     ]
    }
   ],
   "source": [
    "def convert_mon_year_to_datetime(series):\n",
    "    \"Convert 'MMM-YYYY' format to datetime\"\n",
    "    try:\n",
    "        return pd.to_datetime(series, format='%b-%Y')\n",
    "    except:\n",
    "        # Try other common formats\n",
    "        return pd.to_datetime(series, errors='coerce')\n",
    "\n",
    "# Date columns to process (application phase only - NO LEAKAGE)\n",
    "application_dates = ['issue_d', 'earliest_cr_line']\n",
    "\n",
    "date_conversions = {}\n",
    "for date_col in application_dates:\n",
    "    if date_col in accepted_loan.columns:\n",
    "        print(f\"\\nProcessing {date_col}:\")\n",
    "        original_sample = accepted_loan[date_col].dropna().iloc[0] if not accepted_loan[date_col].dropna().empty else 'N/A'\n",
    "        print(f\"  Original format sample: '{original_sample}'\")\n",
    "        \n",
    "        # Convert to datetime\n",
    "        accepted_loan[f'{date_col}_dt'] = convert_mon_year_to_datetime(accepted_loan[date_col])\n",
    "        \n",
    "        # Create useful features\n",
    "        if date_col == 'earliest_cr_line':\n",
    "            # Calculate credit history length in months\n",
    "            if 'issue_d_dt' in accepted_loan.columns:\n",
    "                accepted_loan['credit_history_months'] = (\n",
    "                    (accepted_loan['issue_d_dt'] - accepted_loan[f'{date_col}_dt']) \n",
    "                    / np.timedelta64(1, 'D')\n",
    "                    / 30.4375\n",
    "                ).round()\n",
    "                print(f\"  Created 'credit_history_months' feature\")\n",
    "        \n",
    "        # Drop original string column\n",
    "        accepted_loan.drop(columns=[date_col], inplace=True)\n",
    "        \n",
    "        # Report conversion success\n",
    "        null_pct = accepted_loan[f'{date_col}_dt'].isnull().mean() * 100\n",
    "        print(f\"  Converted, {null_pct:.1f}% null after conversion\")\n",
    "        date_conversions[date_col] = null_pct\n",
    "\n",
    "# Show date ranges\n",
    "if 'issue_d_dt' in accepted_loan.columns:\n",
    "    print(f\"\\nApplication Date Range:\")\n",
    "    print(f\"  Start: {accepted_loan['issue_d_dt'].min()}\")\n",
    "    print(f\"  End: {accepted_loan['issue_d_dt'].max()}\")\n",
    "    print(f\"  Time span: {(accepted_loan['issue_d_dt'].max() - accepted_loan['issue_d_dt'].min()).days / 365:.1f} years\")\n",
    "\n",
    "print(f\"\\nShape after date processing: {accepted_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1832a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with any missing values: 86\n",
      "\n",
      "Missing value summary (sorted by % missing):\n",
      "emp_title                      object        7.4% missing\n",
      "emp_length                     category      6.5% missing\n",
      "open_acc_6m                    float32      38.3% missing\n",
      "open_act_il                    float32      38.3% missing\n",
      "open_il_12m                    float32      38.3% missing\n",
      "open_il_24m                    float32      38.3% missing\n",
      "mths_since_rcnt_il             float32      40.3% missing\n",
      "total_bal_il                   float32      38.3% missing\n",
      "il_util                        float32      47.3% missing\n",
      "open_rv_12m                    float32      38.3% missing\n",
      "open_rv_24m                    float32      38.3% missing\n",
      "max_bal_bc                     float32      38.3% missing\n",
      "all_util                       float32      38.3% missing\n",
      "inq_fi                         float32      38.3% missing\n",
      "total_cu_tl                    float32      38.3% missing\n",
      "inq_last_12m                   float32      38.3% missing\n",
      "mo_sin_old_il_acct             float32       6.2% missing\n",
      "num_tl_120dpd_2m               float32       6.8% missing\n",
      "\n",
      "Overall missingness: 7.11%\n",
      "Missing cells: 15,103,656 / 212,505,706\n"
     ]
    }
   ],
   "source": [
    "# Analyze remaining missing values\n",
    "missing_analysis = accepted_loan.isnull().sum()\n",
    "remaining_missing = missing_analysis[missing_analysis > 0]\n",
    "\n",
    "print(f\"Columns with any missing values: {len(remaining_missing)}\")\n",
    "print(\"\\nMissing value summary (sorted by % missing):\")\n",
    "for col in remaining_missing.index:\n",
    "    missing_pct = (accepted_loan[col].isnull().sum() / len(accepted_loan)) * 100\n",
    "    dtype = accepted_loan[col].dtype\n",
    "    if missing_pct > 5:  # Show only >5% missing\n",
    "        print(f\"{col:30} {str(dtype):10} {missing_pct:6.1f}% missing\")\n",
    "\n",
    "# Calculate overall missingness\n",
    "total_cells = accepted_loan.shape[0] * accepted_loan.shape[1]\n",
    "missing_cells = accepted_loan.isnull().sum().sum()\n",
    "overall_missing_pct = (missing_cells / total_cells) * 100\n",
    "print(f\"\\nOverall missingness: {overall_missing_pct:.2f}%\")\n",
    "print(f\"Missing cells: {missing_cells:,} / {total_cells:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ae59478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary joint app flag\n",
    "if 'is_joint_app' in accepted_loan.columns:\n",
    "    if accepted_loan['is_joint_app'].sum() == 0:\n",
    "        accepted_loan.drop(columns=['is_joint_app'], inplace=True)\n",
    "        print(\"Dropped 'is_joint_app' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae20ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Handling features with temporal introduction pattern (38.3% missing):\n",
      "Found 13 temporal pattern columns\n",
      "  open_acc_6m: Imputed 0.0% missing with 0\n",
      "  open_act_il: Imputed 0.0% missing with 0\n",
      "  open_il_12m: Imputed 0.0% missing with 0\n",
      "  open_il_24m: Imputed 0.0% missing with 0\n",
      "  total_bal_il: Imputed 0.0% missing with 0\n",
      "  il_util: Imputed 0.0% missing with 0\n",
      "  open_rv_12m: Imputed 0.0% missing with 0\n",
      "  open_rv_24m: Imputed 0.0% missing with 0\n",
      "  max_bal_bc: Imputed 0.0% missing with 0\n",
      "  all_util: Imputed 0.0% missing with 0\n",
      "  inq_fi: Imputed 0.0% missing with 0\n",
      "  total_cu_tl: Imputed 0.0% missing with 0\n",
      "  inq_last_12m: Imputed 0.0% missing with 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Handle the 38.3% missing pattern columns (likely introduced later)\n",
    "print(\"\\nHandling features with temporal introduction pattern (38.3% missing):\")\n",
    "temporal_pattern_cols = [\n",
    "    'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m',\n",
    "    'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m',\n",
    "    'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m'\n",
    "]\n",
    "\n",
    "# Check which exist\n",
    "existing_temporal = [col for col in temporal_pattern_cols if col in accepted_loan.columns]\n",
    "print(f\"Found {len(existing_temporal)} temporal pattern columns\")\n",
    "\n",
    "# For these, missing likely means \"not collected yet\" = pre-2014 loans\n",
    "# Impute with 0 (assuming no activity/accounts for these features pre-introduction)\n",
    "for col in existing_temporal:\n",
    "    if accepted_loan[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "        accepted_loan[col] = accepted_loan[col].fillna(0)\n",
    "        print(f\"  {col}: Imputed {accepted_loan[col].isnull().mean()*100:.1f}% missing with 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. mths_since_rcnt_il: Imputed with 999 (no recent installment loan)\n"
     ]
    }
   ],
   "source": [
    "if 'mths_since_rcnt_il' in accepted_loan.columns:\n",
    "    # Missing means \"no recent installment loan\" - impute with large value\n",
    "    accepted_loan['mths_since_rcnt_il'] = accepted_loan['mths_since_rcnt_il'].fillna(999)\n",
    "    print(f\"\\nmths_since_rcnt_il: Imputed with 999 (no recent installment loan)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d65e683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling employment information:\n",
      "  emp_title: Imputed missing as 'Not Provided'\n",
      "  emp_length: Imputed missing as 'Not Provided'\n"
     ]
    }
   ],
   "source": [
    "# 3. Handle emp_title and emp_length (text/category columns)\n",
    "print(\"\\nHandling employment information:\")\n",
    "if 'emp_title' in accepted_loan.columns:\n",
    "    accepted_loan['emp_title'] = accepted_loan['emp_title'].fillna('Not Provided')\n",
    "    print(f\"  emp_title: Imputed missing as 'Not Provided'\")\n",
    "\n",
    "if 'emp_length' in accepted_loan.columns:\n",
    "    # Check if it's category or object\n",
    "    if accepted_loan['emp_length'].dtype.name == 'category':\n",
    "        accepted_loan['emp_length'] = accepted_loan['emp_length'].cat.add_categories(['Not Provided'])\n",
    "    accepted_loan['emp_length'] = accepted_loan['emp_length'].fillna('Not Provided')\n",
    "    print(f\"  emp_length: Imputed missing as 'Not Provided'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec22c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling remaining low missingness columns:\n",
      "  mo_sin_old_il_acct: Imputed 0.0% missing with median 130.00\n",
      "  num_tl_120dpd_2m: Imputed 0.0% missing with median 0.00\n",
      "\n",
      "Shape after strategic imputation: (2260699, 93)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nHandling remaining low missingness columns:\")\n",
    "low_missing_cols = ['mo_sin_old_il_acct', 'num_tl_120dpd_2m']\n",
    "for col in low_missing_cols:\n",
    "    if col in accepted_loan.columns:\n",
    "        if accepted_loan[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "            # Median imputation\n",
    "            median_val = accepted_loan[col].median()\n",
    "            accepted_loan[col] = accepted_loan[col].fillna(median_val)\n",
    "            print(f\"  {col}: Imputed {accepted_loan[col].isnull().mean()*100:.1f}% missing with median {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nShape after strategic imputation: {accepted_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b2d555d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123950 missing values still exist\n",
      "\n",
      "Columns with remaining missing values:\n",
      "  loan_amnt: 31 missing (0.00%)\n",
      "  funded_amnt: 31 missing (0.00%)\n",
      "  term: 31 missing (0.00%)\n",
      "  int_rate: 31 missing (0.00%)\n",
      "  installment: 31 missing (0.00%)\n",
      "  grade: 31 missing (0.00%)\n",
      "  sub_grade: 31 missing (0.00%)\n",
      "  home_ownership: 31 missing (0.00%)\n",
      "  annual_inc: 35 missing (0.00%)\n",
      "  verification_status: 31 missing (0.00%)\n",
      "  loan_status: 31 missing (0.00%)\n",
      "  purpose: 31 missing (0.00%)\n",
      "  title: 23357 missing (1.03%)\n",
      "  zip_code: 32 missing (0.00%)\n",
      "  addr_state: 31 missing (0.00%)\n",
      "  dti: 1742 missing (0.08%)\n",
      "  delinq_2yrs: 60 missing (0.00%)\n",
      "  fico_range_low: 31 missing (0.00%)\n",
      "  fico_range_high: 31 missing (0.00%)\n",
      "  inq_last_6mths: 61 missing (0.00%)\n",
      "  open_acc: 60 missing (0.00%)\n",
      "  pub_rec: 60 missing (0.00%)\n",
      "  revol_bal: 31 missing (0.00%)\n",
      "  revol_util: 1833 missing (0.08%)\n",
      "  total_acc: 60 missing (0.00%)\n",
      "  initial_list_status: 31 missing (0.00%)\n",
      "  policy_code: 31 missing (0.00%)\n",
      "  application_type: 31 missing (0.00%)\n",
      "  acc_now_delinq: 60 missing (0.00%)\n",
      "  tot_coll_amt: 70307 missing (3.11%)\n",
      "  tot_cur_bal: 70307 missing (3.11%)\n",
      "  total_rev_hi_lim: 70307 missing (3.11%)\n",
      "  acc_open_past_24mths: 50061 missing (2.21%)\n",
      "  avg_cur_bal: 70377 missing (3.11%)\n",
      "  bc_open_to_buy: 74966 missing (3.32%)\n",
      "  bc_util: 76102 missing (3.37%)\n",
      "  chargeoff_within_12_mths: 176 missing (0.01%)\n",
      "  delinq_amnt: 60 missing (0.00%)\n",
      "  mo_sin_old_rev_tl_op: 70308 missing (3.11%)\n",
      "  mo_sin_rcnt_rev_tl_op: 70308 missing (3.11%)\n",
      "  mo_sin_rcnt_tl: 70307 missing (3.11%)\n",
      "  mort_acc: 50061 missing (2.21%)\n",
      "  mths_since_recent_bc: 73443 missing (3.25%)\n",
      "  num_accts_ever_120_pd: 70307 missing (3.11%)\n",
      "  num_actv_bc_tl: 70307 missing (3.11%)\n",
      "  num_actv_rev_tl: 70307 missing (3.11%)\n",
      "  num_bc_sats: 58621 missing (2.59%)\n",
      "  num_bc_tl: 70307 missing (3.11%)\n",
      "  num_il_tl: 70307 missing (3.11%)\n",
      "  num_op_rev_tl: 70307 missing (3.11%)\n",
      "  num_rev_accts: 70308 missing (3.11%)\n",
      "  num_rev_tl_bal_gt_0: 70307 missing (3.11%)\n",
      "  num_sats: 58621 missing (2.59%)\n",
      "  num_tl_30dpd: 70307 missing (3.11%)\n",
      "  num_tl_90g_dpd_24m: 70307 missing (3.11%)\n",
      "  num_tl_op_past_12m: 70307 missing (3.11%)\n",
      "  pct_tl_nvr_dlq: 70462 missing (3.12%)\n",
      "  percent_bc_gt_75: 75410 missing (3.34%)\n",
      "  pub_rec_bankruptcies: 1396 missing (0.06%)\n",
      "  tax_liens: 136 missing (0.01%)\n",
      "  tot_hi_cred_lim: 70307 missing (3.11%)\n",
      "  total_bal_ex_mort: 50061 missing (2.21%)\n",
      "  total_bc_limit: 50061 missing (2.21%)\n",
      "  total_il_high_credit_limit: 70307 missing (3.11%)\n",
      "  disbursement_method: 31 missing (0.00%)\n",
      "  issue_d_dt: 31 missing (0.00%)\n",
      "  earliest_cr_line_dt: 60 missing (0.00%)\n",
      "  credit_history_months: 60 missing (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Check if any missing values remain\n",
    "remaining_missing = accepted_loan.isnull().sum().sum()\n",
    "if remaining_missing == 0:\n",
    "    print(\"No missing values remaining!\")\n",
    "else:\n",
    "    print(f\"{remaining_missing} missing values still exist\")\n",
    "    missing_cols = accepted_loan.isnull().sum()\n",
    "    missing_cols = missing_cols[missing_cols > 0]\n",
    "    print(\"\\nColumns with remaining missing values:\")\n",
    "    for col, count in missing_cols.items():\n",
    "        print(f\"  {col}: {count} missing ({count/len(accepted_loan)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71d10adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 categorical columns:\n",
      "id: object, 2260699 unique values\n",
      "\n",
      "term (category, 2 unique):\n",
      "   36 months: 71.2%\n",
      "   60 months: 28.8%\n",
      "\n",
      "grade (category, 7 unique):\n",
      "  B: 29.4%\n",
      "  C: 28.8%\n",
      "  A: 19.2%\n",
      "  D: 14.4%\n",
      "  E: 6.0%\n",
      "sub_grade: category, 35 unique values\n",
      "emp_title: object, 512695 unique values\n",
      "\n",
      "emp_length (category, 12 unique):\n",
      "  10+ years: 33.1%\n",
      "  2 years: 9.0%\n",
      "  < 1 year: 8.4%\n",
      "  3 years: 8.0%\n",
      "  1 year: 6.6%\n",
      "\n",
      "home_ownership (category, 6 unique):\n",
      "  MORTGAGE: 49.2%\n",
      "  RENT: 39.6%\n",
      "  OWN: 11.2%\n",
      "  ANY: 0.0%\n",
      "  OTHER: 0.0%\n",
      "\n",
      "verification_status (category, 3 unique):\n",
      "  Source Verified: 39.2%\n",
      "  Not Verified: 32.9%\n",
      "  Verified: 27.9%\n",
      "\n",
      "loan_status (category, 9 unique):\n",
      "  Fully Paid: 47.6%\n",
      "  Current: 38.9%\n",
      "  Charged Off: 11.9%\n",
      "  Late (31-120 days): 0.9%\n",
      "  In Grace Period: 0.4%\n",
      "\n",
      "purpose (category, 14 unique):\n",
      "  debt_consolidation: 56.5%\n",
      "  credit_card: 22.9%\n",
      "  home_improvement: 6.7%\n",
      "  other: 6.2%\n",
      "  major_purchase: 2.2%\n",
      "title: category, 63154 unique values\n",
      "zip_code: category, 956 unique values\n",
      "addr_state: category, 51 unique values\n",
      "\n",
      "initial_list_status (category, 2 unique):\n",
      "  w: 67.9%\n",
      "  f: 32.1%\n",
      "\n",
      "application_type (category, 2 unique):\n",
      "  Individual: 94.7%\n",
      "  Joint App: 5.3%\n",
      "\n",
      "disbursement_method (category, 2 unique):\n",
      "  Cash: 96.5%\n",
      "  DirectPay: 3.5%\n"
     ]
    }
   ],
   "source": [
    "# Analyze categorical columns for high cardinality\n",
    "categorical_cols = accepted_loan.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(categorical_cols)} categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = accepted_loan[col].nunique()\n",
    "    col_type = accepted_loan[col].dtype\n",
    "    if unique_count < 20:  # Show distribution for low-cardinality\n",
    "        print(f\"\\n{col} ({col_type}, {unique_count} unique):\")\n",
    "        value_counts = accepted_loan[col].value_counts(normalize=True).head(5)\n",
    "        for val, pct in value_counts.items():\n",
    "            print(f\"  {val}: {pct*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{col}: {col_type}, {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "207b2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 constant columns: ['policy_code']\n",
      "Dropped constant column: policy_code\n"
     ]
    }
   ],
   "source": [
    "# Check for constant columns\n",
    "constant_cols = []\n",
    "for col in accepted_loan.columns:\n",
    "    if accepted_loan[col].nunique() == 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"Found {len(constant_cols)} constant columns: {constant_cols}\")\n",
    "else:\n",
    "    print(\"No constant columns found\")\n",
    "\n",
    "# Drop constant column\n",
    "if 'policy_code' in accepted_loan.columns:\n",
    "    accepted_loan.drop(columns=['policy_code'], inplace=True)\n",
    "    print(f\"Dropped constant column: policy_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c079669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped high-cardinality columns: ['emp_title', 'title', 'id', 'zip_code']\n"
     ]
    }
   ],
   "source": [
    "high_cardinality_drops = ['emp_title', 'title', 'id', 'zip_code']  # zip_code has 956 unique, too many\n",
    "existing_high_card = [col for col in high_cardinality_drops if col in accepted_loan.columns]\n",
    "accepted_loan.drop(columns=existing_high_card, inplace=True, errors='ignore')\n",
    "print(f\"Dropped high-cardinality columns: {existing_high_card}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeeeb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying records with multiple missing values...\n",
      "Rows missing >5 values: 70375 (3.11%)\n",
      "Max missing per row: 65\n"
     ]
    }
   ],
   "source": [
    "# Handle the 3.1% missing pattern (likely same records)\n",
    "print(\"\\nIdentifying records with multiple missing values...\")\n",
    "# Count missing per row\n",
    "missing_per_row = accepted_loan.isnull().sum(axis=1)\n",
    "high_missing_rows = missing_per_row[missing_per_row > 5]  # Rows missing >5 values\n",
    "print(f\"Rows missing >5 values: {len(high_missing_rows)} ({len(high_missing_rows)/len(accepted_loan)*100:.2f}%)\")\n",
    "print(f\"Max missing per row: {missing_per_row.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60b1d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loan_amnt: 0.00% → median 12900.00\n",
      "  funded_amnt: 0.00% → median 12875.00\n",
      "  int_rate: 0.00% → median 12.62\n",
      "  installment: 0.00% → median 377.99\n",
      "  annual_inc: 0.00% → median 65000.00\n",
      "  dti: 0.08% → median 17.84\n",
      "  delinq_2yrs: 0.00% → median 0.00\n",
      "  fico_range_low: 0.00% → median 690.00\n",
      "  fico_range_high: 0.00% → median 694.00\n",
      "  inq_last_6mths: 0.00% → median 0.00\n",
      "  open_acc: 0.00% → median 11.00\n",
      "  pub_rec: 0.00% → median 0.00\n",
      "  revol_bal: 0.00% → median 11324.00\n",
      "  revol_util: 0.08% → median 50.30\n",
      "  total_acc: 0.00% → median 22.00\n",
      "  acc_now_delinq: 0.00% → median 0.00\n",
      "  tot_coll_amt: 3.11% → median 0.00\n",
      "  tot_cur_bal: 3.11% → median 79240.00\n",
      "  total_rev_hi_lim: 3.11% → median 25400.00\n",
      "  acc_open_past_24mths: 2.21% → median 4.00\n",
      "  avg_cur_bal: 3.11% → median 7335.00\n",
      "  bc_open_to_buy: 3.32% → median 5442.00\n",
      "  bc_util: 3.37% → median 60.20\n",
      "  chargeoff_within_12_mths: 0.01% → median 0.00\n",
      "  delinq_amnt: 0.00% → median 0.00\n",
      "  mo_sin_old_rev_tl_op: 3.11% → median 164.00\n",
      "  mo_sin_rcnt_rev_tl_op: 3.11% → median 8.00\n",
      "  mo_sin_rcnt_tl: 3.11% → median 6.00\n",
      "  mort_acc: 2.21% → median 1.00\n",
      "  mths_since_recent_bc: 3.25% → median 14.00\n",
      "  num_accts_ever_120_pd: 3.11% → median 0.00\n",
      "  num_actv_bc_tl: 3.11% → median 3.00\n",
      "  num_actv_rev_tl: 3.11% → median 5.00\n",
      "  num_bc_sats: 2.59% → median 4.00\n",
      "  num_bc_tl: 3.11% → median 7.00\n",
      "  num_il_tl: 3.11% → median 6.00\n",
      "  num_op_rev_tl: 3.11% → median 7.00\n",
      "  num_rev_accts: 3.11% → median 12.00\n",
      "  num_rev_tl_bal_gt_0: 3.11% → median 5.00\n",
      "  num_sats: 2.59% → median 11.00\n",
      "  num_tl_30dpd: 3.11% → median 0.00\n",
      "  num_tl_90g_dpd_24m: 3.11% → median 0.00\n",
      "  num_tl_op_past_12m: 3.11% → median 2.00\n",
      "  pct_tl_nvr_dlq: 3.12% → median 100.00\n",
      "  percent_bc_gt_75: 3.34% → median 37.50\n",
      "  pub_rec_bankruptcies: 0.06% → median 0.00\n",
      "  tax_liens: 0.01% → median 0.00\n",
      "  tot_hi_cred_lim: 3.11% → median 114298.50\n",
      "  total_bal_ex_mort: 2.21% → median 37864.00\n",
      "  total_bc_limit: 2.21% → median 16300.00\n",
      "  total_il_high_credit_limit: 3.11% → median 32696.00\n",
      "  credit_history_months: 0.00% → median 178.00\n",
      "  term: 0.00% → mode ' 36 months'\n",
      "  grade: 0.00% → mode 'B'\n",
      "  sub_grade: 0.00% → mode 'C1'\n",
      "  home_ownership: 0.00% → mode 'MORTGAGE'\n",
      "  verification_status: 0.00% → mode 'Source Verified'\n",
      "  loan_status: 0.00% → mode 'Fully Paid'\n",
      "  purpose: 0.00% → mode 'debt_consolidation'\n",
      "  addr_state: 0.00% → mode 'CA'\n",
      "  initial_list_status: 0.00% → mode 'w'\n",
      "  application_type: 0.00% → mode 'Individual'\n",
      "  disbursement_method: 0.00% → mode 'Cash'\n"
     ]
    }
   ],
   "source": [
    "# Group columns by type for appropriate imputation\n",
    "float_cols = accepted_loan.select_dtypes(include=['float32', 'float64']).columns.tolist()\n",
    "cat_cols = accepted_loan.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# For float columns: median imputation\n",
    "for col in float_cols:\n",
    "    if accepted_loan[col].isnull().any():\n",
    "        missing_pct = accepted_loan[col].isnull().mean() * 100\n",
    "        if missing_pct < 5:  # Only for low missing\n",
    "            median_val = accepted_loan[col].median()\n",
    "            accepted_loan[col] = accepted_loan[col].fillna(median_val)\n",
    "            print(f\"  {col}: {missing_pct:.2f}% → median {median_val:.2f}\")\n",
    "\n",
    "# For categorical columns: mode imputation\n",
    "for col in cat_cols:\n",
    "    if accepted_loan[col].isnull().any():\n",
    "        missing_pct = accepted_loan[col].isnull().mean() * 100\n",
    "        if missing_pct < 5:\n",
    "            mode_val = accepted_loan[col].mode()[0] if not accepted_loan[col].mode().empty else 'Unknown'\n",
    "            accepted_loan[col] = accepted_loan[col].fillna(mode_val)\n",
    "            print(f\"  {col}: {missing_pct:.2f}% → mode '{mode_val}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b63206b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining missing values after imputation: 91\n",
      "Dropped 60 rows with missing values\n",
      "\n",
      "Final shape: (2260639, 88)\n",
      "Total missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# 5. Check remaining missing\n",
    "remaining_missing = accepted_loan.isnull().sum().sum()\n",
    "print(f\"\\nRemaining missing values after imputation: {remaining_missing}\")\n",
    "if remaining_missing > 0:\n",
    "    # For any remaining missing, drop those rows (very few)\n",
    "    initial_shape = accepted_loan.shape\n",
    "    accepted_loan = accepted_loan.dropna()\n",
    "    print(f\"Dropped {initial_shape[0] - accepted_loan.shape[0]} rows with missing values\")\n",
    "\n",
    "print(f\"\\nFinal shape: {accepted_loan.shape}\")\n",
    "print(f\"Total missing values: {accepted_loan.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe9b2002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to encode: 12\n"
     ]
    }
   ],
   "source": [
    "cat_cols = accepted_loan.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns to encode: {len(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b08b9d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "term:\n",
      "  Unique values: 2\n",
      "  Top 3 values:\n",
      "    ' 36 months': 71.2%\n",
      "    ' 60 months': 28.8%\n",
      "  Strategy: Binary encoding (0/1)\n",
      "\n",
      "grade:\n",
      "  Unique values: 7\n",
      "  Top 3 values:\n",
      "    'B': 29.4%\n",
      "    'C': 28.8%\n",
      "    'A': 19.2%\n",
      "  Strategy: One-hot encoding\n",
      "\n",
      "sub_grade:\n",
      "  Unique values: 35\n",
      "  Top 3 values:\n",
      "    'C1': 6.5%\n",
      "    'B5': 6.2%\n",
      "    'B4': 6.2%\n",
      "  Strategy: Target encoding (with regularization)\n",
      "\n",
      "emp_length:\n",
      "  Unique values: 12\n",
      "  Top 3 values:\n",
      "    '10+ years': 33.1%\n",
      "    '2 years': 9.0%\n",
      "    '< 1 year': 8.4%\n",
      "  Strategy: Target encoding (with regularization)\n",
      "\n",
      "home_ownership:\n",
      "  Unique values: 6\n",
      "  Top 3 values:\n",
      "    'MORTGAGE': 49.2%\n",
      "    'RENT': 39.6%\n",
      "    'OWN': 11.2%\n",
      "  Strategy: One-hot encoding\n",
      "\n",
      "verification_status:\n",
      "  Unique values: 3\n",
      "  Top 3 values:\n",
      "    'Source Verified': 39.2%\n",
      "    'Not Verified': 32.9%\n",
      "    'Verified': 27.9%\n",
      "  Strategy: One-hot encoding\n",
      "\n",
      "loan_status:\n",
      "  Unique values: 9\n",
      "  Top 3 values:\n",
      "    'Fully Paid': 47.6%\n",
      "    'Current': 38.9%\n",
      "    'Charged Off': 11.9%\n",
      "  Strategy: One-hot encoding\n",
      "\n",
      "purpose:\n",
      "  Unique values: 14\n",
      "  Top 3 values:\n",
      "    'debt_consolidation': 56.5%\n",
      "    'credit_card': 22.9%\n",
      "    'home_improvement': 6.7%\n",
      "  Strategy: Target encoding (with regularization)\n",
      "\n",
      "addr_state:\n",
      "  Unique values: 51\n",
      "  Top 3 values:\n",
      "    'CA': 13.9%\n",
      "    'NY': 8.2%\n",
      "    'TX': 8.2%\n",
      "  Strategy: Frequency encoding\n",
      "\n",
      "initial_list_status:\n",
      "  Unique values: 2\n",
      "  Top 3 values:\n",
      "    'w': 67.9%\n",
      "    'f': 32.1%\n",
      "  Strategy: Binary encoding (0/1)\n",
      "\n",
      "application_type:\n",
      "  Unique values: 2\n",
      "  Top 3 values:\n",
      "    'Individual': 94.7%\n",
      "    'Joint App': 5.3%\n",
      "  Strategy: Binary encoding (0/1)\n",
      "\n",
      "disbursement_method:\n",
      "  Unique values: 2\n",
      "  Top 3 values:\n",
      "    'Cash': 96.5%\n",
      "    'DirectPay': 3.5%\n",
      "  Strategy: Binary encoding (0/1)\n"
     ]
    }
   ],
   "source": [
    "encoding_plan = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    unique_count = accepted_loan[col].nunique()\n",
    "    value_dist = accepted_loan[col].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {unique_count}\")\n",
    "    print(f\"  Top 3 values:\")\n",
    "    for val, pct in value_dist.head(3).items():\n",
    "        print(f\"    '{val}': {pct*100:.1f}%\")\n",
    "    \n",
    "    # Determine encoding strategy\n",
    "    if unique_count == 2:\n",
    "        encoding_plan[col] = 'binary'\n",
    "        print(f\"  Strategy: Binary encoding (0/1)\")\n",
    "    elif unique_count <= 10:\n",
    "        encoding_plan[col] = 'one-hot'\n",
    "        print(f\"  Strategy: One-hot encoding\")\n",
    "    elif unique_count <= 50:\n",
    "        encoding_plan[col] = 'target_encode'\n",
    "        print(f\"  Strategy: Target encoding (with regularization)\")\n",
    "    else:\n",
    "        encoding_plan[col] = 'frequency'\n",
    "        print(f\"  Strategy: Frequency encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ae3cd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datetime columns:\n",
      "  issue_d_dt: Keep as datetime for temporal features\n",
      "  earliest_cr_line_dt: Keep as datetime for temporal features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDatetime columns:\")\n",
    "dt_cols = [col for col in accepted_loan.columns if '_dt' in col]\n",
    "for col in dt_cols:\n",
    "    print(f\"  {col}: Keep as datetime for temporal features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90bb26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features for modeling: 85\n",
      "Features to encode: ['term', 'grade', 'sub_grade', 'emp_length', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status', 'application_type', 'disbursement_method']\n"
     ]
    }
   ],
   "source": [
    "exclude_from_modeling = ['issue_d_dt', 'earliest_cr_line_dt', 'loan_status']  # loan_status for phase 2\n",
    "\n",
    "model_features = [col for col in accepted_loan.columns if col not in exclude_from_modeling]\n",
    "print(f\"Total features for modeling: {len(model_features)}\")\n",
    "print(f\"Features to encode: {[col for col in model_features if col in cat_cols]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9dbdf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'accepted_loan_preprocessed.parquet'\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed data for encoding phase\n",
    "accepted_loan.to_parquet('accepted_loan_preprocessed.parquet', index=False, engine='fastparquet')\n",
    "print(\"Saved to 'accepted_loan_preprocessed.parquet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "243da509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preparing data for tree-based models...\n",
      "  Binary encoded: term → [' 36 months' ' 60 months']\n",
      "  Binary encoded: initial_list_status → ['f' 'w']\n",
      "  Binary encoded: application_type → ['Individual' 'Joint App']\n",
      "  Binary encoded: disbursement_method → ['Cash' 'DirectPay']\n",
      "  Ordinal encoded: emp_length\n",
      "Tree-data shape: (2260639, 88)\n"
     ]
    }
   ],
   "source": [
    "# 1. For tree-based models (Isolation Forest): Can handle some categorical encoding\n",
    "print(\"1. Preparing data for tree-based models...\")\n",
    "\n",
    "# Create a copy for tree-based models\n",
    "tree_data = accepted_loan.copy()\n",
    "\n",
    "# Binary encode 2-value columns\n",
    "binary_cols = [col for col in encoding_plan.keys() if encoding_plan[col] == 'binary']\n",
    "for col in binary_cols:\n",
    "    if col in tree_data.columns:\n",
    "        le = LabelEncoder()\n",
    "        tree_data[col] = le.fit_transform(tree_data[col].astype(str))\n",
    "        print(f\"  Binary encoded: {col} → {le.classes_}\")\n",
    "\n",
    "# Ordinal encode emp_length (has natural order)\n",
    "if 'emp_length' in tree_data.columns:\n",
    "    emp_length_order = ['< 1 year', '1 year', '2 years', '3 years', '4 years', \n",
    "                        '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years']\n",
    "    # Map to ordered categories\n",
    "    tree_data['emp_length'] = pd.Categorical(tree_data['emp_length'], \n",
    "                                             categories=emp_length_order, \n",
    "                                             ordered=True)\n",
    "    tree_data['emp_length_encoded'] = tree_data['emp_length'].cat.codes\n",
    "    tree_data.drop(columns=['emp_length'], inplace=True)\n",
    "    print(f\"  Ordinal encoded: emp_length\")\n",
    "\n",
    "print(f\"Tree-data shape: {tree_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a15e443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for distance-based models...\n",
      "One-hot encoding 3 columns: ['grade', 'home_ownership', 'verification_status']\n",
      "Note: One-hot encoding will be done during model training to avoid memory blow-up\n",
      "\n",
      "Saved tree-ready data: (2260639, 88)\n",
      "Saved distance-ready data: (2260639, 83)\n",
      "\n",
      "PREPROCESSING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# For distance-based models (LOF, SVM): Need proper encoding\n",
    "print(\"\\nPreparing data for distance-based models...\")\n",
    "\n",
    "# Select features for distance-based models\n",
    "distance_features = model_features.copy()\n",
    "# Remove high-cardinality categoricals for distance models\n",
    "high_card_remove = ['sub_grade', 'addr_state']  # Too many categories\n",
    "distance_features = [f for f in distance_features if f not in high_card_remove]\n",
    "\n",
    "# Create dataset with limited categoricals\n",
    "distance_data = accepted_loan[distance_features].copy()\n",
    "\n",
    "# One-hot encode low-cardinality categoricals\n",
    "low_card_cats = [col for col in distance_features \n",
    "                 if col in encoding_plan.keys() \n",
    "                 and encoding_plan[col] == 'one-hot'\n",
    "                 and col not in ['sub_grade', 'addr_state']]\n",
    "\n",
    "print(f\"One-hot encoding {len(low_card_cats)} columns: {low_card_cats}\")\n",
    "\n",
    "# We'll do one-hot encoding separately in modeling phase to avoid memory issues\n",
    "print(\"Note: One-hot encoding will be done during model training to avoid memory blow-up\")\n",
    "\n",
    "# Save both versions\n",
    "tree_data.to_parquet('accepted_loan_tree_ready.parquet', index=False, engine='fastparquet')\n",
    "distance_data.to_parquet('accepted_loan_distance_ready.parquet', index=False, engine='fastparquet')\n",
    "\n",
    "print(f\"\\nSaved tree-ready data: {tree_data.shape}\")\n",
    "print(f\"Saved distance-ready data: {distance_data.shape}\")\n",
    "print(\"\\nPREPROCESSING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
