{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38672fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d01060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (27648741, 9)\n",
      "Columns: ['Amount Requested', 'Application Date', 'Loan Title', 'Risk_Score', 'Debt-To-Income Ratio', 'Zip Code', 'State', 'Employment Length', 'Policy Code']\n",
      "Memory usage: 10795.32 MB\n"
     ]
    }
   ],
   "source": [
    "rejected_loan = pd.read_csv('C:/Users/ayan.pathak\\Desktop/lending club loan data project/Data/rejected_2007_to_2018Q4.csv/rejected_2007_to_2018Q4.csv',low_memory=False)\n",
    "print(f\"Dataset shape: {rejected_loan.shape}\")\n",
    "print(f\"Columns: {rejected_loan.columns.tolist()}\")\n",
    "print(f\"Memory usage: {rejected_loan.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4eabd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Amount Requested Application Date                        Loan Title  \\\n",
      "0            1000.0       2007-05-26  Wedding Covered but No Honeymoon   \n",
      "1            1000.0       2007-05-26                Consolidating Debt   \n",
      "2           11000.0       2007-05-27       Want to consolidate my debt   \n",
      "3            6000.0       2007-05-27                           waksman   \n",
      "4            1500.0       2007-05-27                            mdrigo   \n",
      "\n",
      "   Risk_Score Debt-To-Income Ratio Zip Code State Employment Length  \\\n",
      "0       693.0                  10%    481xx    NM           4 years   \n",
      "1       703.0                  10%    010xx    MA          < 1 year   \n",
      "2       715.0                  10%    212xx    MD            1 year   \n",
      "3       698.0               38.64%    017xx    MA          < 1 year   \n",
      "4       509.0                9.43%    209xx    MD          < 1 year   \n",
      "\n",
      "   Policy Code  \n",
      "0          0.0  \n",
      "1          0.0  \n",
      "2          0.0  \n",
      "3          0.0  \n",
      "4          0.0  \n",
      "object     6\n",
      "float64    3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rejected_loan.head())\n",
    "print(rejected_loan.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f522e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values by column:\n",
      "  Loan Title: 1,305 (0.0%)\n",
      "  Risk_Score: 18,497,630 (66.9%)\n",
      "  Zip Code: 293 (0.0%)\n",
      "  State: 22 (0.0%)\n",
      "  Employment Length: 951,355 (3.4%)\n",
      "  Policy Code: 918 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_summary = rejected_loan.isnull().sum()\n",
    "print(f\"\\nMissing values by column:\")\n",
    "for col, missing_count in missing_summary[missing_summary > 0].items():\n",
    "    missing_pct = missing_count / len(rejected_loan) * 100\n",
    "    print(f\"  {col}: {missing_count:,} ({missing_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c609eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. CATEGORICAL COLUMN ANALYSIS\n",
      "Application Date: 4238 unique values\n",
      "Loan Title: 73927 unique values\n",
      "Debt-To-Income Ratio: 126145 unique values\n",
      "Zip Code: 1001 unique values\n",
      "State: 51 unique values\n",
      "Employment Length: 11 unique values\n"
     ]
    }
   ],
   "source": [
    "# Check unique values for categorical columns\n",
    "print(\"\\n3. CATEGORICAL COLUMN ANALYSIS\")\n",
    "object_cols = rejected_loan.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    unique_count = rejected_loan[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")\n",
    "    if unique_count < 10:\n",
    "        print(f\"  Values: {rejected_loan[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b9e4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. Dropping unusable columns...\n",
      "  Dropping 'Risk_Score' - 66.9% missing, not in accepted data\n",
      "  Columns remaining: ['Amount Requested', 'Application Date', 'Loan Title', 'Debt-To-Income Ratio', 'Zip Code', 'State', 'Employment Length', 'Policy Code']\n",
      "\n",
      "B. Converting high-cardinality objects to category...\n",
      "  Application Date: object â†’ category (saves ~50% memory)\n",
      "  Loan Title: object â†’ category (saves ~50% memory)\n",
      "  Debt-To-Income Ratio: object â†’ category (saves ~50% memory)\n",
      "  Zip Code: object â†’ category (saves ~50% memory)\n",
      "  State: object â†’ category (saves ~50% memory)\n",
      "  Employment Length: object â†’ category (saves ~50% memory)\n",
      "\n",
      "C. Downcasting numeric columns...\n",
      "  Amount Requested: float64\n",
      "\n",
      "Memory after optimization: 810.75 MB\n",
      "Shape: (27648741, 8)\n"
     ]
    }
   ],
   "source": [
    "# A. Immediate column drops\n",
    "print(\"A. Dropping unusable columns...\")\n",
    "cols_to_drop = []\n",
    "if 'Risk_Score' in rejected_loan.columns:\n",
    "    cols_to_drop.append('Risk_Score')\n",
    "    print(f\"  Dropping 'Risk_Score' - 66.9% missing, not in accepted data\")\n",
    "    \n",
    "if 'Policy Code' in rejected_loan.columns:\n",
    "    # Check if constant\n",
    "    if rejected_loan['Policy Code'].nunique() == 1:\n",
    "        cols_to_drop.append('Policy Code')\n",
    "        print(f\"  Dropping 'Policy Code' - constant value\")\n",
    "\n",
    "rejected_loan.drop(columns=cols_to_drop, inplace=True)\n",
    "print(f\"  Columns remaining: {rejected_loan.columns.tolist()}\")\n",
    "\n",
    "# B. Convert object columns to category where beneficial\n",
    "print(\"\\nB. Converting high-cardinality objects to category...\")\n",
    "object_cols = rejected_loan.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    unique_pct = rejected_loan[col].nunique() / len(rejected_loan) * 100\n",
    "    if unique_pct < 50:  # Convert if < 50% unique values\n",
    "        rejected_loan[col] = rejected_loan[col].astype('category')\n",
    "        print(f\"  {col}: object â†’ category (saves ~50% memory)\")\n",
    "\n",
    "# C. Downcast numeric columns\n",
    "print(\"\\nC. Downcasting numeric columns...\")\n",
    "if 'Amount Requested' in rejected_loan.columns:\n",
    "    # Most loan amounts are < 100k, use float32\n",
    "    rejected_loan['Amount Requested'] = pd.to_numeric(\n",
    "        rejected_loan['Amount Requested'], downcast='float'\n",
    "    )\n",
    "    print(f\"  Amount Requested: {rejected_loan['Amount Requested'].dtype}\")\n",
    "\n",
    "print(f\"\\nMemory after optimization: {rejected_loan.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Shape: {rejected_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b52a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing temporal distribution...\n",
      "Yearly distribution:\n",
      "  2007: 5,274 applications (0.0%)\n",
      "  2008: 25,596 applications (0.1%)\n",
      "  2009: 56,991 applications (0.2%)\n",
      "  2010: 112,561 applications (0.4%)\n",
      "  2011: 217,792 applications (0.8%)\n",
      "  2012: 337,277 applications (1.2%)\n",
      "  2013: 760,942 applications (2.8%)\n",
      "  2014: 1,933,700 applications (7.0%)\n",
      "  2015: 2,859,379 applications (10.3%)\n",
      "  2016: 4,769,874 applications (17.3%)\n"
     ]
    }
   ],
   "source": [
    "# First, parse dates to understand temporal distribution\n",
    "if 'Application Date' in rejected_loan.columns:\n",
    "    print(\"Analyzing temporal distribution...\")\n",
    "    rejected_loan['app_date_parsed'] = pd.to_datetime(\n",
    "        rejected_loan['Application Date'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Extract year for stratified sampling\n",
    "    rejected_loan['app_year'] = rejected_loan['app_date_parsed'].dt.year\n",
    "    \n",
    "    print(\"Yearly distribution:\")\n",
    "    year_counts = rejected_loan['app_year'].value_counts().sort_index()\n",
    "    for year, count in year_counts.head(10).items():\n",
    "        if not pd.isna(year):\n",
    "            print(f\"  {year}: {count:,} applications ({count/len(rejected_loan)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1e607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choosing sampling strategy...\n",
      "Option 1: Simple random sample (fast, loses temporal patterns)\n",
      "Option 2: Stratified by year (preserves time distribution)\n",
      "Option 3: Focused on overlapping years with accepted loans\n",
      "\n",
      "Checking temporal overlap with accepted loans...\n"
     ]
    }
   ],
   "source": [
    "# STRATEGIC SAMPLING APPROACH\n",
    "print(\"\\nChoosing sampling strategy...\")\n",
    "print(\"Option 1: Simple random sample (fast, loses temporal patterns)\")\n",
    "print(\"Option 2: Stratified by year (preserves time distribution)\")\n",
    "print(\"Option 3: Focused on overlapping years with accepted loans\")\n",
    "\n",
    "# Check date overlap with accepted loans\n",
    "print(\"\\nChecking temporal overlap with accepted loans...\")\n",
    "# Accepted loans: 2007-06-01 to 2018-12-01 (from your output)\n",
    "accepted_start = pd.Timestamp('2007-06-01')\n",
    "accepted_end = pd.Timestamp('2018-12-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff2d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing application dates...\n",
      "Rejected date range: 2007-05-26 00:00:00 to 2018-12-31 00:00:00\n",
      "Accepted date range: 2007-06-01 00:00:00 to 2018-12-01 00:00:00\n",
      "Applications in accepted time range: 26,834,620 (97.1%)\n"
     ]
    }
   ],
   "source": [
    "# parse dates properly\n",
    "if 'Application Date' in rejected_loan.columns:\n",
    "    print(\"Parsing application dates...\")\n",
    "    # Parse to datetime WITHOUT converting to categorical\n",
    "    rejected_loan['app_date_parsed'] = pd.to_datetime(\n",
    "        rejected_loan['Application Date'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Convert to datetime type explicitly\n",
    "    rejected_loan['app_date_parsed'] = rejected_loan['app_date_parsed'].astype('datetime64[ns]')\n",
    "    \n",
    "    # Get date range (handle NaN values)\n",
    "    valid_dates = rejected_loan['app_date_parsed'].dropna()\n",
    "    if not valid_dates.empty:\n",
    "        rejected_start = valid_dates.min()\n",
    "        rejected_end = valid_dates.max()\n",
    "        print(f\"Rejected date range: {rejected_start} to {rejected_end}\")\n",
    "        \n",
    "        # Check overlap with accepted loans timeframe\n",
    "        accepted_start = pd.Timestamp('2007-06-01')\n",
    "        accepted_end = pd.Timestamp('2018-12-01')\n",
    "        print(f\"Accepted date range: {accepted_start} to {accepted_end}\")\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap_mask = rejected_loan['app_date_parsed'].between(accepted_start, accepted_end)\n",
    "        overlap_count = overlap_mask.sum()\n",
    "        overlap_pct = overlap_count / len(rejected_loan) * 100\n",
    "        print(f\"Applications in accepted time range: {overlap_count:,} ({overlap_pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No valid dates found in Application Date column\")\n",
    "        overlap_mask = pd.Series(False, index=rejected_loan.index)\n",
    "        overlap_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a10736f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current size: 27,648,741\n",
      "Target sample: 2,260,000\n",
      "Sampling ratio: 0.082\n"
     ]
    }
   ],
   "source": [
    "target_sample = 2260000  # Match accepted loans\n",
    "current_size = len(rejected_loan)\n",
    "sampling_ratio = target_sample / current_size\n",
    "\n",
    "print(f\"\\nCurrent size: {current_size:,}\")\n",
    "print(f\"Target sample: {target_sample:,}\")\n",
    "print(f\"Sampling ratio: {sampling_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2d3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sufficient data in overlapping period\n",
      "Stratified sampling by year within overlap period...\n",
      "Sampled 2,193,453 from overlap period\n",
      "\n",
      "Shape after sampling: (2193453, 10)\n"
     ]
    }
   ],
   "source": [
    "if overlap_count >= target_sample * 0.8:  # If sufficient overlap (80%+ of target)\n",
    "    print(\"Sufficient data in overlapping period\")\n",
    "    overlap_data = rejected_loan[overlap_mask].copy()\n",
    "    \n",
    "    if len(overlap_data) > target_sample:\n",
    "        # Add year column for stratified sampling\n",
    "        overlap_data['app_year'] = overlap_data['app_date_parsed'].dt.year\n",
    "        \n",
    "        # Remove rows with NaN years\n",
    "        overlap_data = overlap_data.dropna(subset=['app_year'])\n",
    "        \n",
    "        if overlap_data['app_year'].nunique() > 1:\n",
    "            print(\"Stratified sampling by year within overlap period...\")\n",
    "            # Calculate per-year sampling ratio\n",
    "            sampled_data = overlap_data.groupby('app_year', group_keys=False).apply(\n",
    "                lambda x: x.sample(frac=sampling_ratio, random_state=42, replace=False)\n",
    "            )\n",
    "        else:\n",
    "            # Simple random sample\n",
    "            sampled_data = overlap_data.sample(n=target_sample, random_state=42)\n",
    "        \n",
    "        print(f\"Sampled {len(sampled_data):,} from overlap period\")\n",
    "        rejected_loan = sampled_data\n",
    "    else:\n",
    "        print(f\"Using all overlap data: {len(overlap_data):,} rows\")\n",
    "        rejected_loan = overlap_data\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient overlap, sampling from full dataset\")\n",
    "    \n",
    "    if 'app_date_parsed' in rejected_loan.columns and rejected_loan['app_date_parsed'].notna().any():\n",
    "        # Add year column\n",
    "        rejected_loan['app_year'] = rejected_loan['app_date_parsed'].dt.year\n",
    "        valid_data = rejected_loan.dropna(subset=['app_year'])\n",
    "        \n",
    "        if valid_data['app_year'].nunique() > 1:\n",
    "            print(\"Stratified sampling by year...\")\n",
    "            sampled = valid_data.groupby('app_year', group_keys=False).apply(\n",
    "                lambda x: x.sample(frac=sampling_ratio, random_state=42, replace=False)\n",
    "            )\n",
    "            rejected_loan = sampled\n",
    "        else:\n",
    "            # Simple random sample\n",
    "            rejected_loan = rejected_loan.sample(n=target_sample, random_state=42)\n",
    "    else:\n",
    "        # Simple random sample\n",
    "        rejected_loan = rejected_loan.sample(n=target_sample, random_state=42)\n",
    "\n",
    "print(f\"\\nShape after sampling: {rejected_loan.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25d0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped temporary columns: ['app_year']\n"
     ]
    }
   ],
   "source": [
    "# A. Drop temporary columns\n",
    "cols_to_drop = []\n",
    "if 'app_year' in rejected_loan.columns:\n",
    "    cols_to_drop.append('app_year')\n",
    "if 'app_date_parsed' in rejected_loan.columns:\n",
    "    # We'll keep the parsed date but rename it\n",
    "    pass  # We'll handle this in renaming\n",
    "\n",
    "if cols_to_drop:\n",
    "    rejected_loan.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped temporary columns: {cols_to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94fb54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardizing column names...\n",
      "Renamed columns: {'Amount Requested': 'loan_amnt', 'Application Date': 'application_date', 'Loan Title': 'title', 'Debt-To-Income Ratio': 'dti', 'Zip Code': 'zip_code', 'State': 'addr_state', 'Employment Length': 'emp_length'}\n"
     ]
    }
   ],
   "source": [
    "# Rename columns to match accepted loans naming convention\n",
    "print(\"\\nStandardizing column names...\")\n",
    "column_rename_map = {\n",
    "    'Amount Requested': 'loan_amnt',\n",
    "    'Application Date': 'application_date',  # Will convert to issue_d equivalent\n",
    "    'Loan Title': 'title',\n",
    "    'Debt-To-Income Ratio': 'dti',\n",
    "    'Zip Code': 'zip_code',\n",
    "    'State': 'addr_state',\n",
    "    'Employment Length': 'emp_length'\n",
    "}\n",
    "\n",
    "# Only rename columns that exist\n",
    "existing_renames = {k: v for k, v in column_rename_map.items() if k in rejected_loan.columns}\n",
    "rejected_loan.rename(columns=existing_renames, inplace=True)\n",
    "print(f\"Renamed columns: {existing_renames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44169179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C. Standardizing date column...\n",
      "Date column standardized as 'issue_d_dt'\n",
      "Date range: 2007-06-01 00:00:00 to 2018-12-01 00:00:00\n",
      "Missing dates: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert 'application_date' to match accepted's 'issue_d_dt' format\n",
    "if 'application_date' in rejected_loan.columns:\n",
    "    print(\"\\nC. Standardizing date column...\")\n",
    "    # Convert to datetime (ensure consistency)\n",
    "    rejected_loan['application_date'] = pd.to_datetime(\n",
    "        rejected_loan['application_date'], errors='coerce'\n",
    "    )\n",
    "    # Rename to match accepted format\n",
    "    rejected_loan.rename(columns={'application_date': 'issue_d_dt'}, inplace=True)\n",
    "    print(f\"Date column standardized as 'issue_d_dt'\")\n",
    "    print(f\"Date range: {rejected_loan['issue_d_dt'].min()} to {rejected_loan['issue_d_dt'].max()}\")\n",
    "    print(f\"Missing dates: {rejected_loan['issue_d_dt'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba77ba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning DTI column...\n",
      "  Converting categorical DTI to string...\n",
      "  DTI sample values: ['0.6%', '66.15%', '0%']\n",
      "  DTI statistics after cleaning:\n",
      "    Range: -1.00 to 2376000.00\n",
      "    Mean: 143.90, Median: 19.94\n",
      "    Std: 4032.21\n",
      "  Missing: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Clean 'dti' column (convert percentages to numeric)\n",
    "if 'dti' in rejected_loan.columns:\n",
    "    print(\"\\nCleaning DTI column...\")\n",
    "    \n",
    "    # First, convert categorical back to string for processing\n",
    "    if rejected_loan['dti'].dtype.name == 'category':\n",
    "        print(\"  Converting categorical DTI to string...\")\n",
    "        rejected_loan['dti'] = rejected_loan['dti'].astype(str)\n",
    "    \n",
    "    # Check current format\n",
    "    dti_sample = rejected_loan['dti'].head(3).tolist()\n",
    "    print(f\"  DTI sample values: {dti_sample}\")\n",
    "    \n",
    "    # Function to clean DTI values\n",
    "    def clean_dti_value(val):\n",
    "        if pd.isna(val) or val == 'nan' or val == 'None' or val == 'null':\n",
    "            return np.nan\n",
    "        if isinstance(val, str):\n",
    "            # Remove % sign and spaces\n",
    "            val = val.replace('%', '').strip()\n",
    "            if val == '' or val.lower() == 'nan':\n",
    "                return np.nan\n",
    "            # Handle edge cases\n",
    "            if val.startswith('<'):\n",
    "                val = val[1:]\n",
    "            if val.startswith('>'):\n",
    "                val = val[1:]\n",
    "        try:\n",
    "            float_val = float(val)\n",
    "            return float_val\n",
    "        except (ValueError, TypeError):\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply cleaning\n",
    "    rejected_loan['dti'] = rejected_loan['dti'].apply(clean_dti_value)\n",
    "    \n",
    "    dti_notna = rejected_loan['dti'].dropna()\n",
    "    if not dti_notna.empty:\n",
    "        print(f\"  DTI statistics after cleaning:\")\n",
    "        print(f\"    Range: {dti_notna.min():.2f} to {dti_notna.max():.2f}\")\n",
    "        print(f\"    Mean: {dti_notna.mean():.2f}, Median: {dti_notna.median():.2f}\")\n",
    "        print(f\"    Std: {dti_notna.std():.2f}\")\n",
    "    else:\n",
    "        print(\"  Warning: No valid DTI values after cleaning\")\n",
    "    \n",
    "    missing_count = rejected_loan['dti'].isnull().sum()\n",
    "    missing_pct = missing_count / len(rejected_loan) * 100\n",
    "    print(f\"  Missing: {missing_count:,} ({missing_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9771fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardizing employment length...\n",
      "  Sample employment length values: ['< 1 year' '10+ years' '1 year' '3 years' '5 years']\n",
      "  Employment length distribution after standardization:\n",
      "    < 1 year: 1,823,331 (83.1%)\n",
      "    5 years: 184,003 (8.4%)\n",
      "    NaN: 74,911 (3.4%)\n",
      "    10+ years: 33,514 (1.5%)\n",
      "    1 year: 19,956 (0.9%)\n",
      "    2 years: 15,456 (0.7%)\n",
      "    3 years: 13,601 (0.6%)\n",
      "    4 years: 9,393 (0.4%)\n",
      "    6 years: 5,700 (0.3%)\n",
      "    8 years: 5,264 (0.2%)\n"
     ]
    }
   ],
   "source": [
    "# Standardize 'emp_length' to match accepted format\n",
    "if 'emp_length' in rejected_loan.columns:\n",
    "    print(\"\\nStandardizing employment length...\")\n",
    "    \n",
    "    # Convert to string if categorical\n",
    "    if rejected_loan['emp_length'].dtype.name == 'category':\n",
    "        rejected_loan['emp_length'] = rejected_loan['emp_length'].astype(str)\n",
    "    \n",
    "    # Check current format\n",
    "    unique_emp = rejected_loan['emp_length'].dropna().unique()[:5]\n",
    "    print(f\"  Sample employment length values: {unique_emp}\")\n",
    "    \n",
    "    # Standardization mapping\n",
    "    emp_standardization = {\n",
    "        '< 1 year': '< 1 year',\n",
    "        '1 year': '1 year',\n",
    "        '2 years': '2 years',\n",
    "        '3 years': '3 years',\n",
    "        '4 years': '4 years',\n",
    "        '5 years': '5 years',\n",
    "        '6 years': '6 years',\n",
    "        '7 years': '7 years',\n",
    "        '8 years': '8 years',\n",
    "        '9 years': '9 years',\n",
    "        '10+ years': '10+ years',\n",
    "        '10 years': '10+ years',\n",
    "        '10+years': '10+ years',\n",
    "        '<1 year': '< 1 year',\n",
    "        '1 Year': '1 year',\n",
    "        '2 Years': '2 years',\n",
    "        '3 Years': '3 years',\n",
    "        '4 Years': '4 years',\n",
    "        '5 Years': '5 years',\n",
    "        '6 Years': '6 years',\n",
    "        '7 Years': '7 years',\n",
    "        '8 Years': '8 years',\n",
    "        '9 Years': '9 years',\n",
    "        '10 Years': '10+ years',\n",
    "        'n/a': np.nan,\n",
    "        'nan': np.nan,\n",
    "        'None': np.nan,\n",
    "        '': np.nan\n",
    "    }\n",
    "    \n",
    "    # Apply standardization\n",
    "    rejected_loan['emp_length'] = rejected_loan['emp_length'].map(emp_standardization)\n",
    "    \n",
    "    print(\"  Employment length distribution after standardization:\")\n",
    "    emp_dist = rejected_loan['emp_length'].value_counts(dropna=False).head(10)\n",
    "    for val, count in emp_dist.items():\n",
    "        pct = count / len(rejected_loan) * 100\n",
    "        val_display = 'NaN' if pd.isna(val) else val\n",
    "        print(f\"    {val_display}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "913967d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing values...\n",
      "  Total missing values before imputation: 75,100\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "missing_before = rejected_loan.isnull().sum().sum()\n",
    "print(f\"  Total missing values before imputation: {missing_before:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e3ec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    title: Imputed 0.00% missing with mode 'Debt consolidation'\n",
      "    zip_code: Imputed 0.00% missing with mode '112xx'\n",
      "    Policy Code: Imputed 0.00% missing with median 0.0\n"
     ]
    }
   ],
   "source": [
    "# For columns with minimal missing, use appropriate imputation\n",
    "for col in rejected_loan.columns:\n",
    "    if rejected_loan[col].isnull().any():\n",
    "        missing_pct = rejected_loan[col].isnull().mean()\n",
    "        \n",
    "        if missing_pct < 0.01:  # Less than 1% missing\n",
    "            if rejected_loan[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "                # Numeric column: impute with median\n",
    "                median_val = rejected_loan[col].median()\n",
    "                rejected_loan[col] = rejected_loan[col].fillna(median_val)\n",
    "                print(f\"    {col}: Imputed {missing_pct*100:.2f}% missing with median {median_val}\")\n",
    "            elif rejected_loan[col].dtype == 'object' or rejected_loan[col].dtype.name == 'category':\n",
    "                # Categorical: impute with mode\n",
    "                mode_val = rejected_loan[col].mode()[0] if not rejected_loan[col].mode().empty else 'Unknown'\n",
    "                rejected_loan[col] = rejected_loan[col].fillna(mode_val)\n",
    "                print(f\"    {col}: Imputed {missing_pct*100:.2f}% missing with mode '{mode_val}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "062238f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total missing values after imputation: 74,911\n",
      "\n",
      " FINAL DATASET SUMMARY\n",
      "  Shape: (2193453, 9)\n",
      "  Columns: ['loan_amnt', 'issue_d_dt', 'title', 'dti', 'zip_code', 'addr_state', 'emp_length', 'Policy Code', 'app_date_parsed']\n",
      "  Data types:\n",
      "float64           3\n",
      "datetime64[ns]    2\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "object            1\n",
      "Name: count, dtype: int64\n",
      "  Memory usage: 256.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Final missing check\n",
    "missing_after = rejected_loan.isnull().sum().sum()\n",
    "print(f\"  Total missing values after imputation: {missing_after:,}\")\n",
    "\n",
    "# Final dataset info\n",
    "print(\"\\n FINAL DATASET SUMMARY\")\n",
    "print(f\"  Shape: {rejected_loan.shape}\")\n",
    "print(f\"  Columns: {rejected_loan.columns.tolist()}\")\n",
    "print(f\"  Data types:\")\n",
    "print(rejected_loan.dtypes.value_counts())\n",
    "print(f\"  Memory usage: {rejected_loan.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8292eb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>issue_d_dt</th>\n",
       "      <th>title</th>\n",
       "      <th>dti</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>Policy Code</th>\n",
       "      <th>app_date_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>3500.0</td>\n",
       "      <td>2007-07-14</td>\n",
       "      <td>daniel09</td>\n",
       "      <td>0.60</td>\n",
       "      <td>322xx</td>\n",
       "      <td>FL</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3280</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>2007-11-27</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>66.15</td>\n",
       "      <td>337xx</td>\n",
       "      <td>FL</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>moecheeks21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>674xx</td>\n",
       "      <td>KS</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>2007-12-09</td>\n",
       "      <td>hotsweetmami143</td>\n",
       "      <td>1.50</td>\n",
       "      <td>114xx</td>\n",
       "      <td>NY</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>2007-10-24</td>\n",
       "      <td>Jay3122</td>\n",
       "      <td>5.21</td>\n",
       "      <td>334xx</td>\n",
       "      <td>FL</td>\n",
       "      <td>1 year</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-10-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loan_amnt issue_d_dt             title    dti zip_code addr_state  \\\n",
       "471      3500.0 2007-07-14          daniel09   0.60    322xx         FL   \n",
       "3280     5000.0 2007-11-27  home_improvement  66.15    337xx         FL   \n",
       "72       2500.0 2007-06-02       moecheeks21   0.00    674xx         KS   \n",
       "4083     1500.0 2007-12-09   hotsweetmami143   1.50    114xx         NY   \n",
       "2006    15000.0 2007-10-24           Jay3122   5.21    334xx         FL   \n",
       "\n",
       "     emp_length  Policy Code app_date_parsed  \n",
       "471    < 1 year          0.0      2007-07-14  \n",
       "3280  10+ years          0.0      2007-11-27  \n",
       "72     < 1 year          0.0      2007-06-02  \n",
       "4083   < 1 year          0.0      2007-12-09  \n",
       "2006     1 year          0.0      2007-10-24  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "663765a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. CRITICAL DTI DATA QUALITY INVESTIGATION\n",
      "\n",
      "A. DTI Distribution Analysis:\n",
      "DTI value ranges:\n",
      "  0-10: 535,460 (24.41%)\n",
      "  10-20: 466,027 (21.25%)\n",
      "  20-30: 370,177 (16.88%)\n",
      "  30-40: 251,414 (11.46%)\n",
      "  40-50: 155,425 (7.09%)\n",
      "  50-100: 252,089 (11.49%)\n",
      "  100-500: 44,028 (2.01%)\n",
      "  500-1k: 7,007 (0.32%)\n",
      "  1k-5k: 3,245 (0.15%)\n",
      "  5k-10k: 6,864 (0.31%)\n",
      "  10k-50k: 2,560 (0.12%)\n",
      "  50k-100k: 791 (0.04%)\n",
      "  100k-1M: 171 (0.01%)\n",
      "  >1M: 3 (0.00%)\n",
      "\n",
      "B. Investigating extreme DTI values:\n",
      "Sample rows with DTI > 100:\n",
      "  Loan: $10,000, DTI: 146, Title: MTM21o83\n",
      "  Loan: $5,000, DTI: 271, Title: michele\n",
      "  Loan: $5,000, DTI: 2,527, Title: CRodney\n",
      "  Loan: $7,500, DTI: 175, Title: CinLou114\n",
      "  Loan: $2,000, DTI: 239, Title: thejuug\n",
      "\n",
      "C. Checking for data type issues:\n",
      "DTI dtype: float64\n",
      "\n",
      "Examining raw DTI string patterns (before conversion):\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. CRITICAL DTI DATA QUALITY INVESTIGATION\")\n",
    "\n",
    "# Investigate DTI distribution\n",
    "if 'dti' in rejected_loan.columns:\n",
    "    print(\"\\nA. DTI Distribution Analysis:\")\n",
    "    \n",
    "    # Check value counts for common ranges\n",
    "    print(\"DTI value ranges:\")\n",
    "    bins = [0, 10, 20, 30, 40, 50, 100, 500, 1000, 5000, 10000, 50000, 100000, 1000000, float('inf')]\n",
    "    labels = ['0-10', '10-20', '20-30', '30-40', '40-50', '50-100', '100-500', '500-1k', \n",
    "              '1k-5k', '5k-10k', '10k-50k', '50k-100k', '100k-1M', '>1M']\n",
    "    \n",
    "    dti_ranges = pd.cut(rejected_loan['dti'], bins=bins, labels=labels, include_lowest=True)\n",
    "    range_counts = dti_ranges.value_counts().sort_index()\n",
    "    \n",
    "    for rng, count in range_counts.items():\n",
    "        if count > 0:\n",
    "            pct = count / len(rejected_loan) * 100\n",
    "            print(f\"  {rng}: {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Investigate extreme values\n",
    "    print(\"\\nB. Investigating extreme DTI values:\")\n",
    "    \n",
    "    # Check if DTI might be mislabeled (maybe it's income or loan amount?)\n",
    "    # Compare with loan_amnt to see correlation\n",
    "    if 'loan_amnt' in rejected_loan.columns:\n",
    "        # Sample extreme DTI rows\n",
    "        extreme_dti = rejected_loan[rejected_loan['dti'] > 100].head(5)\n",
    "        print(\"Sample rows with DTI > 100:\")\n",
    "        for idx, row in extreme_dti.iterrows():\n",
    "            print(f\"  Loan: ${row['loan_amnt']:,.0f}, DTI: {row['dti']:,.0f}, Title: {row.get('title', 'N/A')}\")\n",
    "    \n",
    "    # Check for data type confusion\n",
    "    print(\"\\nC. Checking for data type issues:\")\n",
    "    print(f\"DTI dtype: {rejected_loan['dti'].dtype}\")\n",
    "    \n",
    "    # Look at the raw values before cleaning\n",
    "    print(\"\\nExamining raw DTI string patterns (before conversion):\")\n",
    "    # We need to reload or examine original data pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "516cdd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. INTELLIGENT DTI PROCESSING\n",
      "DTI Category Distribution:\n",
      "  0: 180,853 (8.25%)\n",
      "  0-10: 452,799 (20.64%)\n",
      "  10-20: 466,027 (21.25%)\n",
      "  20-30: 370,177 (16.88%)\n",
      "  30-40: 251,414 (11.46%)\n",
      "  40-50: 155,425 (7.09%)\n",
      "  50-100: 252,089 (11.49%)\n",
      "  >100: 64,669 (2.95%)\n",
      "\n",
      "Extreme DTI (>100) flag: 64,669 rows (2.95%)\n",
      "\n",
      "DTI Statistics (capped at 100):\n",
      "  Min: 0.00\n",
      "  25%: 8.02\n",
      "  Median: 19.94\n",
      "  75%: 36.50\n",
      "  Max: 100.00\n",
      "  Mean: 27.43\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Intelligent DTI Processing\n",
    "print(\"\\n8. INTELLIGENT DTI PROCESSING\")\n",
    "\n",
    "# Create DTI categories based on distribution\n",
    "rejected_loan['dti_category'] = pd.cut(\n",
    "    rejected_loan['dti'],\n",
    "    bins=[-1, 0, 10, 20, 30, 40, 50, 100, float('inf')],\n",
    "    labels=['0', '0-10', '10-20', '20-30', '30-40', '40-50', '50-100', '>100'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(\"DTI Category Distribution:\")\n",
    "category_counts = rejected_loan['dti_category'].value_counts().sort_index()\n",
    "for cat, count in category_counts.items():\n",
    "    pct = count / len(rejected_loan) * 100\n",
    "    print(f\"  {cat}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Create flag for extreme DTI values\n",
    "rejected_loan['dti_extreme'] = (rejected_loan['dti'] > 100).astype(int)\n",
    "print(f\"\\nExtreme DTI (>100) flag: {rejected_loan['dti_extreme'].sum():,} rows ({rejected_loan['dti_extreme'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Cap DTI at 100 for modeling, but keep original for reference\n",
    "rejected_loan['dti_capped'] = rejected_loan['dti'].clip(upper=100, lower=0)\n",
    "\n",
    "print(\"\\nDTI Statistics (capped at 100):\")\n",
    "capped_stats = rejected_loan['dti_capped'].describe()\n",
    "print(f\"  Min: {capped_stats['min']:.2f}\")\n",
    "print(f\"  25%: {capped_stats['25%']:.2f}\")\n",
    "print(f\"  Median: {capped_stats['50%']:.2f}\")\n",
    "print(f\"  75%: {capped_stats['75%']:.2f}\")\n",
    "print(f\"  Max: {capped_stats['max']:.2f}\")\n",
    "print(f\"  Mean: {capped_stats['mean']:.2f}\")\n",
    "\n",
    "# Keep original DTI for anomaly investigation\n",
    "rejected_loan['dti_original'] = rejected_loan['dti']\n",
    "rejected_loan['dti'] = rejected_loan['dti_capped']  # Use capped for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06b0433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EMPLOYMENT LENGTH CORRECTION & FINAL CLEANUP\n",
      "A. Validating employment length distribution...\n",
      "\n",
      "Accepted loans employment length distribution:\n",
      "  1 year: 6.6%\n",
      "  10+ years: 33.1%\n",
      "  2 years: 9.0%\n",
      "  3 years: 8.0%\n",
      "  4 years: 6.0%\n",
      "  5 years: 6.2%\n",
      "  6 years: 4.5%\n",
      "  7 years: 4.1%\n",
      "  8 years: 4.1%\n",
      "  9 years: 3.5%\n",
      "  < 1 year: 8.4%\n",
      "  Not Provided: 6.5%\n",
      "\n",
      "B. Creating employment length categories...\n",
      "Employment length categories:\n",
      "  0-1 years: 83.1%\n",
      "  1-3 years: 2.2%\n",
      "  10+ years: 1.5%\n",
      "  4-6 years: 9.1%\n",
      "  7-9 years: 0.6%\n",
      "  Unknown: 3.4%\n",
      "\n",
      "C. Final column selection...\n",
      "Selected 8 columns for anomaly detection:\n",
      "  ['loan_amnt', 'issue_d_dt', 'dti', 'dti_category', 'dti_extreme', 'addr_state', 'emp_length_category', 'title']\n",
      "\n",
      "D. Saving final datasets...\n",
      "Saved full cleaned dataset: 'rejected_loan_full_cleaned.parquet'\n",
      "Saved modeling-ready dataset: 'rejected_loan_modeling_ready.parquet'\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "1. Total rows: 2,193,453\n",
      "2. Features: 8\n",
      "3. Memory: 241.63 MB\n",
      "4. Time range: 2007-06-01 00:00:00 to 2018-12-01 00:00:00\n",
      "\n",
      "5. Data Quality Metrics:\n",
      "   DTI >100 values: 2.95% (anomaly candidates)\n",
      "   Unknown employment: 3.42%\n",
      "\n",
      " REJECTED DATA READY FOR ANOMALY DETECTION!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n EMPLOYMENT LENGTH CORRECTION & FINAL CLEANUP\")\n",
    "\n",
    "# A. Compare with accepted loans distribution\n",
    "print(\"A. Validating employment length distribution...\")\n",
    "\n",
    "# Load accepted loans for comparison\n",
    "try:\n",
    "    accepted_data = pd.read_parquet('accepted_loan_preprocessed.parquet', engine='fastparquet', columns=['emp_length'])\n",
    "    print(\"\\nAccepted loans employment length distribution:\")\n",
    "    accepted_dist = accepted_data['emp_length'].value_counts(normalize=True).sort_index() * 100\n",
    "    for val, pct in accepted_dist.items():\n",
    "        print(f\"  {val}: {pct:.1f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load accepted data: {e}\")\n",
    "    print(\"Using domain knowledge: '10+ years' should be largest category\")\n",
    "\n",
    "# B. Create corrected employment categories\n",
    "print(\"\\nB. Creating employment length categories...\")\n",
    "\n",
    "# Current distribution seems off (83% < 1 year is unrealistic)\n",
    "# Create broader categories\n",
    "employment_mapping = {\n",
    "    '< 1 year': '0-1 years',\n",
    "    '1 year': '1-3 years',\n",
    "    '2 years': '1-3 years',\n",
    "    '3 years': '1-3 years',\n",
    "    '4 years': '4-6 years',\n",
    "    '5 years': '4-6 years',\n",
    "    '6 years': '4-6 years',\n",
    "    '7 years': '7-9 years',\n",
    "    '8 years': '7-9 years',\n",
    "    '9 years': '7-9 years',\n",
    "    '10+ years': '10+ years',\n",
    "    np.nan: 'Unknown'\n",
    "}\n",
    "\n",
    "rejected_loan['emp_length_category'] = rejected_loan['emp_length'].map(employment_mapping)\n",
    "\n",
    "print(\"Employment length categories:\")\n",
    "cat_dist = rejected_loan['emp_length_category'].value_counts(normalize=True).sort_index() * 100\n",
    "for cat, pct in cat_dist.items():\n",
    "    print(f\"  {cat}: {pct:.1f}%\")\n",
    "\n",
    "# C. Final column cleanup\n",
    "print(\"\\nC. Final column selection...\")\n",
    "\n",
    "# Select columns for anomaly detection\n",
    "final_columns = [\n",
    "    'loan_amnt',\n",
    "    'issue_d_dt',\n",
    "    'dti',  # Capped version (0-100)\n",
    "    'dti_category',\n",
    "    'dti_extreme',  # Flag for >100\n",
    "    'addr_state',\n",
    "    'emp_length_category',\n",
    "    'title'  # Keep for potential text analysis\n",
    "]\n",
    "\n",
    "# Add any other relevant columns\n",
    "available_final = [col for col in final_columns if col in rejected_loan.columns]\n",
    "print(f\"Selected {len(available_final)} columns for anomaly detection:\")\n",
    "print(f\"  {available_final}\")\n",
    "\n",
    "# Create final dataset\n",
    "rejected_final = rejected_loan[available_final].copy()\n",
    "\n",
    "# D. Save final datasets\n",
    "print(\"\\nD. Saving final datasets...\")\n",
    "\n",
    "# Save full cleaned dataset\n",
    "rejected_loan.to_parquet('rejected_loan_full_cleaned.parquet', engine='fastparquet')\n",
    "print(\"Saved full cleaned dataset: 'rejected_loan_full_cleaned.parquet'\")\n",
    "\n",
    "# Save modeling-ready dataset\n",
    "rejected_final.to_parquet('rejected_loan_modeling_ready.parquet', engine='fastparquet')\n",
    "print(\"Saved modeling-ready dataset: 'rejected_loan_modeling_ready.parquet'\")\n",
    "\n",
    "# E. Final summary\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"1. Total rows: {len(rejected_final):,}\")\n",
    "print(f\"2. Features: {rejected_final.shape[1]}\")\n",
    "print(f\"3. Memory: {rejected_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"4. Time range: {rejected_final['issue_d_dt'].min()} to {rejected_final['issue_d_dt'].max()}\")\n",
    "\n",
    "# Show data quality metrics\n",
    "print(\"\\n5. Data Quality Metrics:\")\n",
    "if 'dti_extreme' in rejected_final.columns:\n",
    "    extreme_pct = rejected_final['dti_extreme'].mean() * 100\n",
    "    print(f\"   DTI >100 values: {extreme_pct:.2f}% (anomaly candidates)\")\n",
    "\n",
    "if 'emp_length_category' in rejected_final.columns:\n",
    "    unknown_emp = (rejected_final['emp_length_category'] == 'Unknown').mean() * 100\n",
    "    print(f\"   Unknown employment: {unknown_emp:.2f}%\")\n",
    "\n",
    "print(\"\\n REJECTED DATA READY FOR ANOMALY DETECTION!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61ef16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROJECT PATH VERIFICATION ===\n",
      "  âœ… Notebook Folder: Directory\n",
      "  âœ… Project Folder: Directory\n",
      "  âœ… Raw Data Directory: Directory\n",
      "  âœ… Raw Accepted Data: 1597.5 MB\n",
      "  âœ… Raw Rejected Data: 1699.7 MB\n",
      "  âœ… Processed Data Directory: Directory\n",
      "  âœ… Processed Accepted: 293.3 MB\n",
      "  âœ… Processed Rejected: 40.4 MB\n",
      "\n",
      "ðŸŽ‰ ALL CRITICAL PATHS VERIFIED!\n",
      "Ready for anomaly detection.\n",
      "ðŸ“‚ Loading accepted_preprocessed...\n",
      "   Path: c:\\\\Users\\\\ayan.pathak\\\\Desktop\\\\lending club loan data project\\\\Notebook\\processed_data\\accepted\\accepted_loan_preprocessed.parquet\n",
      "âœ… Loaded: 2,260,639 rows Ã— 88 columns\n",
      "   Memory: 715.77 MB\n",
      "Shape: (2260639, 88)\n",
      "Loan status distribution:\n",
      "loan_status\n",
      "Fully Paid            0.476304\n",
      "Current               0.388526\n",
      "Charged Off           0.118798\n",
      "Late (31-120 days)    0.009496\n",
      "In Grace Period       0.003732\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Test config\n",
    "from config import check_paths\n",
    "check_paths()\n",
    "\n",
    "# Cell 2: Load data\n",
    "from config import load_dataset\n",
    "accepted_data = load_dataset('accepted_preprocessed')\n",
    "\n",
    "# Cell 3: Quick check\n",
    "print(f\"Shape: {accepted_data.shape}\")\n",
    "if 'loan_status' in accepted_data.columns:\n",
    "    print(\"Loan status distribution:\")\n",
    "    print(accepted_data['loan_status'].value_counts(normalize=True).head())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
